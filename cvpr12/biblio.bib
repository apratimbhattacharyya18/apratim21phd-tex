@InProceedings{ryoo09iccv,
  title =	"Spatio-temporal relationship match: Video structure
		 comparison for recognition of complex human
		 activities",
  author =	"Michael S. Ryoo and Jake K. Aggarwal",
  year = 	"2009",
   booktitle =	"ICCV",
 
}

@InProceedings{sung11corr,
  title =	"Human Activity Detection from {RGBD} Images",
  author =	"Jaeyong Sung and Colin Ponce and Bart Selman and
		 Ashutosh Saxena",
  booktitle =	"AAAI Workshop",
  year = 	"2011",
  bibdate =	"2011-08-04",
  bibsource =	"DBLP,
		 http://dblp.uni-trier.de/db/journals/corr/corr1107.html#abs-1107-0169",
  URL =  	"http://arxiv.org/abs/1107.0169",
}

@InProceedings{yuan09cvpr,
  author =	"J. S. Yuan and Z. C. Liu and Y. Wu",
  title =	"Discriminative subvolume search for efficient action
		 detection",
  booktitle =	"CVPR",
  year = 	"2009",
  URL =  	"http://dx.doi.org/10.1109/CVPRW.2009.5206671",
  bibsource =	"http://www.visionbib.com/bibliography/motion-f728.html#TT64009",
}

@Article{yuan11pami,
  title =	"Discriminative Video Pattern Search for Efficient
		 Action Detection",
  author =	"Junsong Yuan and Zicheng Liu and Ying Wu",
  journal =	"IEEE Trans. Pattern Anal. Mach. Intell",
  year = 	"2011",
  number =	"9",
  volume =	"33",
  bibdate =	"2011-10-24",
  bibsource =	"DBLP,
		 http://dblp.uni-trier.de/db/journals/pami/pami33.html#YuanLW11",
  URL =  	"http://doi.ieeecomputersociety.org/10.1109/TPAMI.2011.38",
}

@InProceedings{patron10bmvc,
  title =	"High Five: Recognising human interactions in {TV}
		 shows",
  author =	"Alonso Patron-Perez and Marcin Marszalek and Andrew
		 Zisserman and Ian D. Reid",
  bibdate =	"2011-08-20",
  bibsource =	"DBLP,
		 http://dblp.uni-trier.de/db/conf/bmvc/bmvc2010.html#PatronMZR10",
  booktitle =	"BMVC",
  year = 	"2010",
  ISBN = 	"1-901725-40-5",
  URL =  	"http://dx.doi.org/10.5244/C.24.50",
}


@InProceedings{oh11cvpr,
  title =	"A large-scale benchmark dataset for event recognition
		 in surveillance video",
  author =	"Sangmin Oh and et al.",
    year = 	"2011",
  bibdate =	"2011-08-23",
  bibsource =	"DBLP,
		 http://dblp.uni-trier.de/db/conf/cvpr/cvpr2011.html#OhHPCCLMALDSWJRSVPRYTSFRD11",
  booktitle =	"CVPR",
  URL =  	"http://dx.doi.org/10.1109/CVPR.2011.5995586",
}

@InProceedings{ryoo11iccv,
  title =	"Human Activity Prediction: Early Recognition of Ongoing Activities from Streaming Videos",
  author =	"Michael S. Ryoo",
  year = 	"2011",
  booktitle =	"ICCV",
 
}

@inproceedings{vedaldi10cvpr,
  Author    = {A. Vedaldi and A. Zisserman},
  Title     = {Efficient Additive Kernels via
               Explicit Feature Maps},
  Booktitle = {CVPR},
  Year      = {2010}}

@InProceedings{liu09cvpr,
  author =	"J. G. Liu and J. B. Luo and M. Shah",
  title =	"Recognizing realistic actions from videos 'in the
		 wild'",
  booktitle =	"CVPR",
  year = 	"2009",
  URL =  	"http://dx.doi.org/10.1109/CVPRW.2009.5206744",
  bibsource =	"http://www.visionbib.com/bibliography/motion-f740.html#TT65838",
}

@InProceedings{rodriguez08cvpr,
  author =	"M. D. Rodriguez and J. Ahmed and M. Shah",
  title =	"Action {MACH} a spatio-temporal Maximum Average
		 Correlation Height filter for action recognition",
  booktitle =	"CVPR",
  year = 	"2008",
  URL =  	"http://dx.doi.org/10.1109/CVPR.2008.4587727",
  bibsource =	"http://www.visionbib.com/bibliography/motion-f740.html#TT65834",
}

@conference {rohrbach11cvpr,
	title = {Evaluating Knowledge Transfer and Zero-Shot Learning in a Large-Scale Setting},
	booktitle = {CVPR},
	year = {2011},
	author = {Rohrbach, Marcus and Michael Stark and Schiele, Bernt}
}


@Article{gorelick07pami,
  author =	"L. Gorelick and M. Blank and E. Shechtman and M. Irani
		 and R. Basri",
  title =	"Actions as Space-Time Shapes",
  journal =	"IEEE Trans. Pattern Analysis and Machine
		 Intelligence",
  volume =	"29",
  year = 	"2007",
  number =	"12",
  month =	dec,
  URL =  	"http://dx.doi.org/10.1109/TPAMI.2007.70711",
  bibsource =	"http://www.visionbib.com/bibliography/motion-f741.html#TT65962",
}

@Proceedings{ferryman07pets,
  title = 	 {PETS},
  year = 	 {2007},
editor = 	 {James M. Ferryman},
}

@InProceedings{klaser10sga,
  title =	"Human Focused Action Localization in Video",
  author =	"Alexander Klaser and Marcin Marsza{\l}ek and Cordelia Schmid and Andrew Zisserman",
  year = 	"2010",
   booktitle =	"SGA Workshop",
}


@InProceedings{duchenne09iccv,
  title =	"Automatic annotation of human actions in video",
  author =	"Olivier Duchenne and Ivan Laptev and Josef Sivic and
		 Francis Bach and Jean Ponce",
  year = 	"2009",
  bibdate =	"2011-01-30",
  bibsource =	"DBLP,
		 http://dblp.uni-trier.de/db/conf/iccv/iccv2009.html#DuchenneLSBP09",
  booktitle =	"ICCV",
  URL =  	"http://dx.doi.org/10.1109/ICCV.2009.5459279",
}
@InProceedings{gaidon11cvpr,
  author = 	 {Adrien Gaidon AND Zaid Harchaoui AND Cordelia Schmid},
  title = 	 {Actom Sequence Models for Efficient Action Detection},
 booktitle = {CVPR},
    year = 	 {2011},
}

@InProceedings{kuehne11iccv,
  author = 	 {H. Kuehne AND H. Jhuang AND E. Garrote AND T. Poggio AND T. Serre},
  title = 	 {HMDB: A Large Video Database for Human Motion Recognition},
 booktitle = {ICCV},
    year = 	 {2011},
}

@InProceedings{gall10eccv,
  title =	"2{D} Action Recognition Serves 3{D} Human Pose Estimation",
  author =	"Juergen Gall and Angela Yao and Luc J. Van Gool",
  bibdate =	"2010-09-27",
  bibsource =	"DBLP,
		 http://dblp.uni-trier.de/db/conf/eccv/eccv2010-3.html#GallYG10",
  booktitle =	"ECCV",
  publisher =	"Springer",
  year = 	"2010",
  volume =	"6313",
  editor =	"Kostas Daniilidis and Petros Maragos and Nikos
		 Paragios",
  ISBN = 	"978-3-642-15557-4",
  series =	"Lecture Notes in Computer Science",
  URL =  	"http://dx.doi.org/10.1007/978-3-642-15558-1",
}
   
@INPROCEEDINGS{ahad11sice, 
author={Ahad, Md. Atiqur Rahman and Tan, J. and Kim, H. and Ishikawa, S.}, 
booktitle={SICE}, 
title={Action dataset - A survey}, 
year={2011}, 
abstract={Human action understanding and recognition have various demands for different applications in the field of computer vision and human-machine interaction. Due to these issues, more than a decade, extensive researches are going on in this arena #x2014; to recognize various actions and activities. Researchers have been exploiting various action datasets and some of them become prominent. Though there are some good datasets, unfortunately, to have a strong survey on these datasets has been a long due. This paper attempts this and presents the key datasets and analyzes them in different perspectives.}, 
} 

@INPROCEEDINGS{roggen10icnss,
  title =	"Collecting complex activity data sets in highly rich
		 networked sensor environments",
  author =	"Daniel Roggen and et al.",
	booktitle = "ICNSS",
  year = 	"2010",
  abstract =	"We deployed 72 sensors of 10 modalities in 15 wireless
		 and wired networked sensor systems in the environment,
		 in objects, and on the body to create a sensor-rich
		 environment for the machine recognition of human
		 activities. We acquired data from 12 subjects
		 performing morning activities, yielding over 25 hours
		 of sensor data. We report the number of activity
		 occurrences observed during post-processing, and
		 estimate that over 13000 and 14000 object and
		 environment interactions occurred. We describe the
		 networked sensor setup and the methodology for data
		 acquisition, synchronization and curation. We report on
		 the challenges and outline lessons learned and best
		 practice for similar large scale deployments of
		 heterogeneous networked sensor systems. We evaluate
		 data acquisition quality for on-body and object
		 integrated wireless sensors; there is less than 2.5%
		 packet loss after tuning. We outline our use of the
		 dataset to develop new sensor network self-organization
		 principles and machine learning techniques for activity
		 recognition in opportunistic sensor configurations.
		 Eventually this dataset will be made public.",
  bibsource =	"OAI-PMH server at infoscience.epfl.ch",
  language =	"en",
  oai =  	"oai:infoscience.epfl.ch:148362",
  subject =	"[Opportunity]",
  URL =  	"http://infoscience.epfl.ch/record/148362",
}

@INPROCEEDINGS{marszalek09cvpr, 
author={Marszalek, M. and Laptev, I. and Schmid, C.}, 
booktitle={CVPR}, 
title={Actions in context}, 
year={2009}, 
abstract={This paper exploits the context of natural dynamic scenes for human action recognition in video. Human actions are frequently constrained by the purpose and the physical properties of scenes and demonstrate high correlation with particular scene classes. For example, eating often happens in a kitchen while running is more common outdoors. The contribution of this paper is three-fold: (a) we automatically discover relevant scene classes and their correlation with human actions, (b) we show how to learn selected scene classes from video without manual supervision and (c) we develop a joint framework for action and scene recognition and demonstrate improved recognition of both in natural video. We use movie scripts as a means of automatic supervision for training. For selected action classes we identify correlated scene classes in text and then retrieve video samples of actions and scenes for training using script-to-video alignment. Our visual models for scenes and actions are formulated within the bag-of-features framework and are combined in a joint scene-action SVM-based classifier. We report experimental results and validate the method on a new large dataset with twelve action classes and ten scene classes acquired from 69 movies.}, 
keywords={SVM-based classifier;automatic supervision;human action recognition;manual supervision;movie scripts;natural dynamic scenes;natural video;scene recognition;script-to-video alignment;gesture recognition;image classification;natural scenes;support vector machines;video signal processing;}, 
doi={10.1109/CVPR.2009.5206557}, 
ISSN={1063-6919},}




@INPROCEEDINGS{messing09iccv, 
author={Messing, R. and Pal, C. and Kautz, H.}, 
booktitle={ICCV}, 
year=2009,
title={Activity recognition using the velocity histories of tracked keypoints}, 
keywords={KTH activity recognition dataset;generative mixture model;high resolution video sequences;human psychophysical performance;latent velocity model;local spatio-temporal features;tracked keypoint velocity history;image recognition;image sequences;}, 
}


@inproceedings{wang09bmvc,
    hal_id = {inria-00439769},
    url = {http://hal.inria.fr/inria-00439769/en/},
    title = {{Evaluation of local spatio-temporal features for action recognition}},
    author = {Wang, Heng and Ullah, Muhammad and Klaser, Alexander and Laptev, Ivan and Schmid, Cordelia},
    abstract = {{Local space-time features have recently become a popular video representation for action recognition. Several methods for feature localization and description have been proposed in the literature and promising recognition results were demonstrated for a number of action classes. The comparison of existing methods, however, is often limited given the different experimental settings used. The purpose of this paper is to evaluate and compare previously proposed space-time features in a common experimental setup. In particular, we consider four different feature detectors and six local feature descriptors and use a standard bag-of-features SVM approach for action recognition. We investigate the performance of these methods on a total of 25 action classes distributed over three datasets with varying difficulty. Among interesting conclusions, we demonstrate that regular sampling of space-time features consistently outperforms all tested space-time interest point detectors for human actions in realistic settings. We also demonstrate a consistent ranking for the majority of methods over different datasets and discuss their advantages and limitations.}},
    keywords = {descriptor, evaluation, action, recognition, classification, SVM, BOW, BOF, 3D, videos, KTH, Hollywood2, Hollywood, UCF sports},
    language = {Anglais},
    affiliation = {LEAR - INRIA Grenoble Rh{\^o}ne-Alpes / LJK Laboratoire Jean Kuntzmann - CNRS : FR71 - CNRS : UMR5527 - INRIA - Laboratoire Jean Kuntzmann - Universit{\'e} Joseph Fourier - Grenoble I - Institut National Polytechnique de Grenoble - INPG - Pattern - LIAMA - CIRAD - CNRS - INRA - INRIA - Chinese Academy of Science (CAS) - Institute of Automation, Chinese Academy of Sciences - VISTAS - INRIA - IRISA - INRIA - Institut National des Sciences Appliqu{\'e}es de Rennes - CNRS : UMR6074 - Universit{\'e} de Rennes I - {\'E}cole normale sup{\'e}rieure de Cachan - ENS Cachan},
    booktitle = {{BMVC'09}},
    pdf = {http://hal.inria.fr/inria-00439769/PDF/paper.pdf},
}

@InProceedings{laptev07iccv,
  title =	"Retrieving actions in movies",
  author =	"Ivan Laptev and Patrick P{\'e}rez",
  year = 	"2007",
  bibdate =	"2008-11-04",
  bibsource =	"DBLP,
		 http://dblp.uni-trier.de/db/conf/iccv/iccv2007.html#LaptevP07",
  booktitle =	"ICCV",
  crossref =	"conf/iccv/2007",
  URL =  	"http://dx.doi.org/10.1109/ICCV.2007.4409105",
}


@INPROCEEDINGS{laptev08cvpr, 
author={Laptev, I. and Marszalek, M. and Schmid, C. and Rozenfeld, B.}, 
booktitle={CVPR}, 
year=2008,
title={Learning realistic human actions from movies}, 
abstract={The aim of this paper is to address recognition of natural human actions in diverse and realistic video settings. This challenging but important subject has mostly been ignored in the past due to several problems one of which is the lack of realistic and annotated video datasets. Our first contribution is to address this limitation and to investigate the use of movie scripts for automatic annotation of human actions in videos. We evaluate alternative methods for action retrieval from scripts and show benefits of a text-based classifier. Using the retrieved action samples for visual learning, we next turn to the problem of action classification in video. We present a new method for video classification that builds upon and extends several recent ideas including local space-time features, space-time pyramids and multi-channel non-linear SVMs. The method is shown to improve state-of-the-art results on the standard KTH action dataset by achieving 91.8% accuracy. Given the inherent problem of noisy labels in automatic annotation, we particularly investigate and show high tolerance of our method to annotation errors in the training set. We finally apply the method to learning and classifying challenging action classes in movies and show promising results.}, 
keywords={automatic video annotation;human action retrieval;local space-time feature;movie script;multichannel nonlinear SVM;space-time pyramid;text-based classifier;video action classification;video realistic human action recognition;visual learning;cinematography;image classification;image retrieval;learning (artificial intelligence);support vector machines;video signal processing;}, 
}

@INPROCEEDINGS{yeffet09iccv, 
author={Yeffet, L. and Wolf, L.}, 
booktitle={ICCV}, 
title={Local Trinary Patterns for human action recognition}, 
year={2009}, 
abstract={We present a novel action recognition method which is based on combining the effective description properties of Local Binary Patterns with the appearance invariance and adaptability of patch matching based methods. The resulting method is extremely efficient, and thus is suitable for real-time uses of simultaneous recovery of human action of several lengths and starting points. Tested on all publicity available datasets in the literature known to us, our system repeatedly achieves state of the art performance. Lastly, we present a new benchmark that focuses on uncut motion recognition in broadcast sports video.}, 
keywords={appearance adaptability;appearance invariance;broadcast sports video;datasets;human action recognition method;human action recovery;local binary patterns;local trinary patterns;motion recognition;patch matching based methods;pattern matching;video surveillance;}, 
doi={10.1109/ICCV.2009.5459201}, 
ISSN={1550-5499},}

@InProceedings{laptev05ijcv,
  author = 	 {Ivan Laptev},
  title = 	 {On Space-Time Interest Points},
booktitle={IJCV},
  year={2005},
}

@misc{everingham11pascal,
	author = "Everingham, M. and Van~Gool, L. and Williams, C. K. I. and Winn, J. and Zisserman, A.",
	title = "The {PASCAL} Action Classification Taster Competition",
	year = {2011},
}	

@InProceedings{dalal06eccv,
  author =	"N. Dalal and B. Triggs and C. Schmid",
  title =	"Human Detection Using Oriented Histograms of Flow and Appearance",
  booktitle =	"ECCV",
  year = 	"2006",
  URL =  	"http://dx.doi.org/10.1007/11744047_33",
  bibsource =	"http://www.visionbib.com/bibliography/motion-f730.html#TT64278",
}


@InProceedings{natarajan08cvpr,
  author = 	 {P. Natarajan and R. Nevatia},
  title = 	 {View and scale invariant action recognition using multiview shape-flow models},
  booktitle = {CVPR},
  year = 	 {2008},
 }

@InProceedings{taralova11iccv,
  author = 	 {Ekaterina Taralova and Fernando De la Torre and Martial Hebert},
  title = 	 {Source Constrained Clustering},
  booktitle = {ICCV},
  year = 	 {2011},
 }

@InProceedings{taralova09ev,
  author =	"E. H. Spriggs and F. de la Torre and M. Hebert",
  title =	"Temporal segmentation and activity classification from
		 first-person sensing",
  booktitle =	"Egoc.Vis.'09",
}
@InProceedings{singh11iccv,
  author = 	 {Vivek Singh AND Ram Nevatia},
  title = 	 {Action Recognition in Cluttered Dynamic Scenes using Pose-Specific Part Models},
  booktitle = {ICCV},
  year = 	 {2011},
 }


@InProceedings{nga11iccv,
  author = 	 {Do Hang Nga AND Keiji Yanai},
  title = 	 {Automatic Construction of an Action Video Shot Database using Web Videos},
  booktitle = {ICCV},
  year = 	 {2011},
 }


@InProceedings{chakraborty11iccv,
  author = 	 {Bhaskar Chakraborty AND Michael B. Holte AND Thomas B. Moeslund AND Jordi Gonzalez and F. Xavier Roca },
  title = 	 {A Selective Spatio-Temporal Interest Point Detector for Human Action Recognition in Complex Scenes},
  booktitle = {ICCV},
  year = 	 {2011},
 }

@InProceedings{si11iccv,
  author = {Zhangzhang Sia AND Mingtao Peib AND Benjamin Yaoa AND Song-Chun Zhua},
  title = {Unsupervised Learning of Event AND-OR Grammar and Semantics from Video},
  booktitle = {ICCV},
  year = 	 {2011},  
}

@InProceedings{brendel11iccv,
  author = 	 {William Brendel and Sinisa Todorovic},
  title = 	 {Learning Spatiotemporal Graphs of Human Activities},
  booktitle = {ICCV},
  year = 	 {2011},
}
@InProceedings{yao11iccv,
  author = 	 {Bangpeng Yao AND Xiaoye Jiang AND Aditya Khosla AND Andy Lai Lin AND Leonidas Guibas AND Li Fei-Fei1},
  title = 	 {Human Action Recognition by Learning Bases of Action Attributes and Parts},
    booktitle = {ICCV},
  year = 	 {2011},
}

@INPROCEEDINGS{gehrig09hr, 
author={Gehrig, D. and Kuehne, H. and Woerner, A. and Schultz, T.}, 
booktitle={Humanoid Robots'09},
title={HMM-based human motion recognition with optical flow data}, 
keywords={HMM;HMM-based human motion recognition;complex motion sequences;hidden Markov models;high-level hierarchy;marker-based features;optical flow data;optical flow gradient histograms;pose estimation;recognition process;rule-based formulation;feature extraction;hidden Markov models;image sequences;motion estimation;robot vision;statistical analysis;}, 
doi={10.1109/ICHR.2009.5379546}, 
}

@InProceedings{aubert07mm,
  title =	"Advene: an open-source framework for integrating and
		 visualising audiovisual metadata",
  author =	"Olivier Aubert and Yannick Pri{\'e}",
  bibdate =	"2007-10-24",
  bibsource =	"DBLP,
		 http://dblp.uni-trier.de/db/conf/mm/mm2007.html#AubertP07",
  booktitle =	"ACM Multimedia",
  year = 	"2007",
  ISBN = 	"978-1-59593-702-5",
  URL =  	"http://doi.acm.org/10.1145/1291233.1291451",
}

@TechReport{torre09tr,
  author = 	 {F. De la Torre and J. Hodgins and J. Montano and S. Valcarcel and R. Forcada and J. Macey},
  title = 	 {Guide to the CMU Multimodal Activity Database},
  institution =  {Robotics Institute},
  number = 	 {CMU-RI-TR-08-22},
}
 
@article{aggarwal11acs,
 author = {Aggarwal, J.K. and Ryoo, M.S.},
 title = {Human activity analysis: A review},
 journal = {ACM Comput. Surv.},
 issue_date = {2011},
 volume = {43},
 issue = {3},
 month = apr,
 year = {2011},
 issn = {0360-0300},
 articleno = {16},
 url = {http://doi.acm.org/10.1145/1922649.1922653},
 doi = {http://doi.acm.org/10.1145/1922649.1922653},
 acmid = {1922653},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Computer vision, activity analysis, event detection, human activity recognition, video recognition},
} 

@Article{ward11tist, 
  title =	"Performance metrics for activity recognition",
  author =	"Jamie A. Ward and Paul Lukowicz and Hans-Werner Gellersen",
  journal =	"ACM TIST",
  year = 	"2011",
  number =	"1",
  volume =	"2",
  bibdate =	"2011-01-20",
}

@InProceedings{schuldt04icpr,
  author =	"C. Schuldt and I. Laptev and B. Caputo",
  title =	"Recognizing human actions: a local {SVM} approach",
  booktitle =	"ICPR",
  year = 	"2004",
  URL =  	"http://dx.doi.org/10.1109/ICPR.2004.1334462",
  bibsource =	"http://www.visionbib.com/bibliography/motion-f740.html#TT65751",
}

@InProceedings{tenorth09iccw,
  author =       {Moritz Tenorth and Jan Bandouch and Michael Beetz},
  title =        {{The {TUM} Kitchen Data Set of Everyday Manipulation Activities for Motion Tracking and Action Recognition}},
  booktitle =    {THEMIS},
  year =         {2009},
  bib2html_pubtype ={Conference Paper},
  bib2html_rescat  = {Perception},
  bib2html_groups = {Memoman, K4C},
  bib2html_funding = {CoTeSys},
  bib2html_domain  = {Assistive Household},
  abstract =     {We introduce the publicly available TUM Kitchen Data Set as a comprehensive collection of activity sequences recorded in a kitchen environment equipped with multiple complementary sensors. The recorded data consists of observations of naturally performed manipulation tasks as encountered in everyday activities of human life. Several instances of a table-setting task were performed by different subjects, involving the manipulation of objects and the environment. We provide the original video sequences, fullbody motion capture data recorded by a markerless motion tracker, RFID tag readings and magnetic sensor readings from objects and the environment, as well as corresponding action labels. In this paper, we both describe how the data was computed, in particular the motion tracker and the labeling, and give examples what it can be used for. We present first results of an automatic method for segmenting the observed motions into semantic classes, and describe how the data can be integrated in a knowledge-based framework for reasoning about the observations.},
}


@inproceedings{zinnen09iswc,
	Author = {Andreas Zinnen and Ulf Blanke and Bernt Schiele},
	Booktitle = {ISWC},
	Date-Added = {2009-09-18 11:02:00 +0200},
	Date-Modified = {2010-10-31 12:32:00 +0100},
	Title = {An Analysis of Sensor-Oriented vs. Model-Based Activity Recognition},
	Year = {2009},
	}
	
@inproceedings {zinnen09loca,
	title = {Multi Activity Recognition Based on Bodymodel-Derived Primitives},
	booktitle = {LoCA},
	year = {2009},
	author = {Zinnen, Andreas and Christian Wojek and Schiele, Bernt}
}

@InProceedings{wang11cvpr,
  AUTHOR = {Heng Wang and Alexander Kl{\"a}ser and Cordelia Schmid and Cheng-Lin Liu},
  TITLE = {{Action Recognition by Dense Trajectories}},
  BOOKTITLE = {CVPR},
  YEAR = {2011},
  }

@InProceedings{niebles10eccv,
   author = {Niebles, Juan and Chen, Chih-Wei and Fei-Fei, Li},
   title = {Modeling Temporal Structure of Decomposable Motion Segments for Activity Classification},
   booktitle = {ECCV 2010},
}
  
@conference {Andriluka:2009,
	title = {Pictorial Structures Revisited: People Detection and Articulated Pose Estimation},
	booktitle = {CVPR},
	year = {2009},
	url = {http://www.mis.tu-darmstadt.de/node/381},
	attachments = {http://www.d2.mpi-inf.mpg.de/sites/default/files/andriluka_cvpr09.pdf},
	author = {Mykhaylo Andriluka and Stefan Roth and Schiele, Bernt}
}  


@inproceedings{confcvprYangR11,
  author = {Yang, Yi and Ramanan, Deva},
  booktitle = {CVPR},
  interhash = {01f5b10931fe27f0642c63855ec5207b},
  intrahash = {0abb6f28a20a8b80708df99950556d04},
  title = {Articulated pose estimation with flexible mixtures-of-parts.},
  url = {http://dblp.uni-trier.de/db/conf/cvpr/cvpr2011.html#YangR11},
  year = 2011,
  timestamp = {2011-08-23T00:00:00.000+0200},
  keywords = {dblp},
  ee = {http://dx.doi.org/10.1109/CVPR.2011.5995741},
  added-at = {2011-08-23T00:00:00.000+0200},
  biburl = {http://www.bibsonomy.org/bibtex/20abb6f28a20a8b80708df99950556d04/dblp}
}

@InProceedings{Sapp10cascadedmodels,
    author = {Benjamin Sapp and Alexander Toshev and Ben Taskar},
    title = {Cascaded Models for Articulated Pose Estimation},
    year = {2010},
    booktitle = {ECCV},
}

@InProceedings{Ferrari:2008:PSS,
  author = 	 {Vittorio Ferrari and Manuel Marin and Andrew Zisserman},
  title = 	 {Progressive Search Space Reduction for Human Pose Estimation},
  booktitle = 	 cvpr-2008,
  year =	 cvpr-2008-yr,
}
                  
@InProceedings{Sapp:2011:PHM,
  author = 	 {Ben Sapp and David Weiss and Ben Taskar},
  title = 	 {Parsing Human Motion with Stretchable Models},
  booktitle = 	 cvpr-2011,
  year =	 cvpr-2011-yr,
}


@ARTICLE{Felzenszwalb2010PAMI,
author = {P. F. Felzenszwalb and R. B. Girshick and D. McAllester and D. Ramanan},
title = {Object Detection with Discriminatively Trained Part-Based Models},
journal = {PAMI},
volume = {32},
year = {2010},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
}
                  
@InProceedings{Yang:2010:RHA,
  author = 	 {Weilong Yang and Yang Wang and Greg Mori},
  title = 	 {Recognizing Human Actions from Still Images with Latent Poses},
  booktitle = 	 cvpr-2010,
  year =	 cvpr-2010-yr,
}
